# All things Model Interpretability (MLI) #

## Model Interpretability Projects ##
 - [CriticalML: Resources for FAT/MLI](https://github.com/rockita/criticalML)
 - [DataScience: Skater](https://github.com/datascienceinc/Skater)
 - [Lime](https://github.com/marcotcr/lime)
 - [FairML: Auditing Black-Box Predictive Models](http://blog.fastforwardlabs.com/2017/03/09/fairml-auditing-black-box-predictive-models.html)
 - [ChaLearn AutoML challenge ](http://automl.chalearn.org/)
 - [DEvol - Deep Neural Network Evolution](https://github.com/joeddav/devol)
 - [SHAP (SHapley Additive exPlanations) - Explain the output of any machine learning model using expectations and Shapley values](https://github.com/slundberg/shap)
 - [H2O MLI Resources](https://github.com/h2oai/mli-resources)
 - [ExploreKit: Java based ML Explanation](https://github.com/giladkatz/ExploreKit)
 - [XGBoost Decision Paths](https://github.com/MLWave/black-boxxy#xgboost-decision-paths)
 
## Source Code/Jupyter Notebook ##
 - [LIME Test](https://github.com/DustinVerzal/lime/blob/master/doc/notebooks/Lime%20%20-%20H2O%20Example.ipynb)

## Model Interpretability Articles ##
 - [Ideas on interpreting machine learning](https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning)
 - [Interpreting Random Forest](http://blog.datadive.net/interpreting-random-forests/)
 - [Look Who's Fighting Our Algorithmic Overlords](https://www.bloomberg.com/view/articles/2017-08-30/look-who-s-fighting-our-algorithmic-overlords)
 - [How AI detectives are cracking open the black box of deep learning](http://www.sciencemag.org/news/2017/07/how-ai-detectives-are-cracking-open-black-box-deep-learning)
 - [Interpretability in conversation with Patrick Hall and Sameer Singh](http://blog.fastforwardlabs.com/2017/09/11/interpretability-webinar.html)
